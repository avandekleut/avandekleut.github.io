{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "permalink: /dqn/\n",
    "layout: single\n",
    "author_profile: true\n",
    "title: Deep Q Learning with PyTorch\n",
    "folder: \"dqn\"\n",
    "ipynb: \"dqn.ipynb\"\n",
    "md: \"dqn.md\"\n",
    "excerpt: ########################\n",
    "header:\n",
    "  teaser: /assets/###################\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my undergraduate thesis, I spent months learning the intricacies of deep reinforcement learning. I ended up [doing research](thesis.pdf) on intrinsic motivation using [sparse distributed memory](https://en.wikipedia.org/wiki/Sparse_distributed_memory) as a kind of hashing function to determine novelty for encouraging exploration.\n",
    "\n",
    "While I was doing research for my honour's thesis, I found that a lot of the existing materials on deep reinforcement learning were outdated and over-complicated. I decided to make this post to help explain the fundamentals of deep reinforcement learning, and to provide a reference implementation based on the principles described in the [seminal paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) on deep RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical foundations of reinforcement learning\n",
    "\n",
    "All of reinforcement learning is based on the **reward hypothesis**:\n",
    "\n",
    "> Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal.\n",
    "\n",
    "We usually think of reinforcement learning scenarios as consisting of an **agent** and an **environment**, which we formalize using the notion of a **Markov decision process** (MDP). We will build up an understanding of MDPs in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Process $(\\mathcal{S}, \\mathcal{P})$\n",
    "\n",
    "A **Markov process** is a kind of mathematical object used to model stochastic sequences of states $s_0, s_1, \\dots, s_T$ that satisfy the **Markov property**:\n",
    "$$\n",
    "p(s_{t+1} \\mid s_1, s_2, \\dots, s_t) = p(s_{t+1} \\mid s_t)\n",
    "$$\n",
    "\n",
    "which, translated, states that\n",
    "\n",
    "> The probability of transitioning to a state $s_{t+1}$ given a current state $s_t$ is independent of previous states $s_1 \\dots s_{t-1}$.\n",
    "\n",
    "The subscript $t$ usually means \"at some time step $t$\".\n",
    "\n",
    "Formally, a Markov process is a tuple $(\\mathcal{S}, \\mathcal{P})$ where $\\mathcal{S}$ is a set of states $s$ and $\\mathcal{P}(s_t, s_{t+1})$ is probability of transitioning from a state $s_t$ to a state $s_{t+1}$.\n",
    "\n",
    "Given some initial state $s_0$, we can generate a **trajectory** $\\tau$ of states of $\\{ s_0, s_1, \\dots, s_T \\}$. \n",
    "\n",
    "To help understand a Markov process, consider how to calculate the probability of an arbitrary trajectory. We can calculate the probability of a trajectory $\\tau$ by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\tau) &= p(s_0) p(s_1 \\mid s_0) p(s_2 \\mid s_1) \\dots p(s_T \\mid s_{T-1}) \\\\\n",
    "&= p(s_0) \\prod_{t=0}^{T-1} p(s_{t+1} \\mid s_t) \\\\\n",
    "&= p(s_0) \\prod_{t=0}^{T-1} \\mathcal{P} (s_t, s_{t+1})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\mathcal{S}$ is discrete, we can represent $\\mathcal{P}$ using a table. Each $\\mathcal{P}_{i, j} = p(s_j \\mid s_i)$ is just the probability of transitioning to state $s_j$ from state $s_i$.\n",
    "\n",
    "We also additionally specify a probability distribution over initial state $p(s_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to create a `MarkovProcess` class. \n",
    "\n",
    "The class should know the number of discrete states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess:\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.S = range(N)\n",
    "        P = [np.random.rand(N) for _ in range(N)]\n",
    "        self.P = np.vstack([p/p.sum() for p in P])  # normalize\n",
    "        p_s_0 = np.random.rand(N)\n",
    "        self.p_s_0 = p_s_0/p_s_0.sum() # distribution over s_0\n",
    "    \n",
    "    def generate_trajectory(self, T):\n",
    "        s_0 = np.random.choice(self.S, p=self.p_s_0)\n",
    "        tau = [s_0]\n",
    "        s_t = s_0\n",
    "        for t in range(T):\n",
    "            # discrete probability distribution over next states  \n",
    "            s_t = np.random.choice(self.S, p=self.P[s_t])\n",
    "            tau.append(s_t)\n",
    "        return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19356424, 0.25224431, 0.21259213, 0.19217803, 0.14942128],\n",
       "       [0.19440831, 0.13170981, 0.26841564, 0.29005381, 0.11541244],\n",
       "       [0.27439982, 0.18330691, 0.19687558, 0.32079767, 0.02462001],\n",
       "       [0.03366488, 0.00781195, 0.32170632, 0.30066296, 0.3361539 ],\n",
       "       [0.31185458, 0.25466645, 0.14705881, 0.24872985, 0.0376903 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = MarkovProcess(5)\n",
    "mp.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns `S[i]` of the transition matrix give the probabilities of transition to another state `j`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03366488, 0.00781195, 0.32170632, 0.30066296, 0.3361539 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.P[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.P[3].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this transition matrix to sample trajectories, given an initial state $s_0$ and number of timesteps $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3, 3, 0, 2, 2, 2, 3, 4, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.generate_trajectory(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand what a Markov process is, we can add the next bit: rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process $(\\mathcal{S}, \\mathcal{P}, \\mathcal{R})$\n",
    "\n",
    "A Markov reward process is a Markov process that additionally defines a **reward function** $\\mathcal{R}$ over state transitions. Given a state transition $(s_t, s_{t+1})$ from our Markov process, we get a scalar reward $r_t$ indicating how \"good\" that transition was. For example, let's say $s_t$ is standing at the edge of a cliff. If $s_{t+1}$ is off the cliff, then we would get a low reward. If $s_{t+1}$ is back from the edge of the cliff, we get a high reward.\n",
    "\n",
    "Technically, the reward function defines a *distribution* of rewards for any given state transition. When we run the Markov reward process (i.e., when we generate trajectories) we sample from that distribution to get our **instantaneous reward** $r_t$. The definition for the reward function is often given as\n",
    "$$\n",
    "\\mathcal{R}(s_t, s_{t+1}) = \\mathbb{E}[r_t \\mid s_t, s_{t+1}]\n",
    "$$\n",
    "the mean of the distribution. \n",
    "\n",
    "Oftentimes the reward is produced deterministically, so that\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(s_t, s_{t+1}) = r_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to simplify analysis, we sometimes simply associate the reward $r_t$ with the state $s_{t+1}$. In this way, the initial state $s_0$ is not associated with a reward, but each subsequent state $s_1, s_2, \\dots, s_T$ is. The idea is that usually, the goodness moving from state $s_t$ to state $s_{t+1}$ is determined by how good $s_{t+1}$ is. This is often how Markov reward processes are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a trajectory of state $\\{ s_0, s_1, \\dots, s_T \\}$ we can associate a trajectory of rewards $\\{ r_1, r_2, \\dots, r_T \\}$. The **return** of a trajectory starting at time step $t$ is the sum of the instantaneous rewards from $t$ to $T$ (the end of the trajectory).\n",
    "$$\n",
    "R_t = r_t + r_{t+1} + \\dots + r_T = \\sum_{k=t}^T r_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good trajectories are ones with high returns. \n",
    "\n",
    "So far it has been implicit that $T$ (the length of the trajectory) is finite. However, this does not have to be the case. In general, trajectories can continue forever (i.e., $T = \\infty$). When a Markov process has finite $T$ we say that it is \"episodic\" (trajectories happen in \"episodes\"), and if has infinite $T$ we say that it is \"continuous\". For this reason, we generally refer to $T$ as the **time horizon** for the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This poses a problem. If the process is continuous, then there can generally be many trajectories with the same, infinite return $R_t$. If one trajectories sees rewards of $\\{ 1, 1, 1, \\dots \\}$ and one sees rewards of $\\{ 100, 100, 100, \\dots \\}$, they will get the same return $R_t$. However, clearly the second one is better. To get around this, we often use the **discounted return** $G_t$:\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} = \\sum_{k=t}^T \\gamma^{k-t} r_k\n",
    "$$\n",
    "\n",
    "which discounts each instantenous reward $r_k$ by some **discount factor** $\\gamma \\in (0, 1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ controls how much rewards in the future influence $G_t$. For large values of $\\gamma$, the sequence $\\gamma, \\gamma^2, \\gamma^3, \\dots$ decays slowly, so rewards far into the future are weighted more. Likewise, for small values of $\\gamma$, that sequence decays quickly, so rewards far into the future are weighted less. In practice, we often take  $\\gamma = 0.99$ or $\\gamma = 0.999$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we usually implement Markov reward processes by making the reward dependent only on the subsequent state in a state transition, i.e. $\\mathcal{R}(s_t, s_{t+1}) = \\mathcal{R}(s_{t+1})$. This means that the reward only depends on where you end up and not how you got there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement $\\mathcal{R}$ for a discrete state space using the approach above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess(MarkovProcess):\n",
    "    def __init__(self, N):\n",
    "        super(MarkovRewardProcess, self).__init__(N)\n",
    "        self.R = np.random.randn(N)\n",
    "    \n",
    "    def generate_trajectory(self, T):\n",
    "        s_0 = np.random.choice(self.S, p=self.p_s_0)\n",
    "        tau_s = [s_0]\n",
    "        tau_r = []\n",
    "        s_t = s_0\n",
    "        for t in range(T):  \n",
    "            s_t = np.random.choice(self.S, p=self.P[s_t])\n",
    "            r_t = self.R[s_t]\n",
    "            tau_s.append(s_t)\n",
    "            tau_r.append(r_t)\n",
    "        return tau_s, tau_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp = MarkovRewardProcess(5)\n",
    "tau_s, tau_r = mrp.generate_trajectory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 0, 3, 2, 1, 3, 0, 3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.027971183469780222,\n",
       " 0.6467800971387857,\n",
       " 0.42539339846964996,\n",
       " 0.6467800971387857,\n",
       " 1.4759898324822718,\n",
       " -0.027971183469780222,\n",
       " 0.6467800971387857,\n",
       " 0.42539339846964996,\n",
       " 0.6467800971387857,\n",
       " -0.3643343119343121]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine how good being in a certain state $s_t$ is using the **value function** $V(s_t)$:\n",
    "$$\n",
    "V(s_t) = \\mathbb{E}_\\tau [G_t \\mid s_t]\n",
    "$$\n",
    "the expected discounted return beginning at state $s_t$. We write the expectation over $\\tau$ as a shorthand for $\\tau \\sim p(\\tau \\mid s_t, \\mathcal{S})$, which means \"trajectories $\\tau$ sampled from the distribution of trajectories conditional on the initial state $s_t$ and the state transition matrix $\\mathcal{S}$\".\n",
    "\n",
    "We can expand $G_t$ in the expectation to yield a recursive formulation of $V(s_t)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s_t) &=  \\mathbb{E}_\\tau [G_t \\mid s_t] \\\\\n",
    "&=  \\mathbb{E}_\\tau [r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\mid s_t] \\\\\n",
    "&=  \\mathbb{E}_\\tau [r_t + \\gamma (r_{t+1} + \\gamma r_{t+2} + \\dots) \\mid s_t] \\\\\n",
    "&=  \\mathbb{E}_\\tau [r_t + \\gamma G_{t+1} \\mid s_t] \\\\\n",
    "&=  \\mathbb{E}_\\tau [r_t + \\gamma V(s_{t+1}) \\mid s_t] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last formulation is known as the **Bellman equation**. We can use it to get an estimate for $V(s_t)$.\n",
    "\n",
    "Imagine we are in a state $s_t$, and then transition to a state $s_{t+1}$, receiving a reward $r_t$. Imagine we have an estimate for $V(s)$ for each $s \\in \\mathcal{S}$. After this state transition, we have a slightly better estimate for $V(s_t)$, namely $r_t + \\gamma V(s_{t+1})$. Then we can use a simple update rule like\n",
    "\n",
    "$$\n",
    "V(s_t) \\gets (1 - \\alpha) V(s_t) + \\alpha (r_t + \\gamma V(s_{t+1})\n",
    "$$\n",
    "\n",
    "to move our estimate for $V(s_t)$ towards the slightly better estimate, where $\\alpha \\in (0, 1]$ is a **learning rate** (often close to $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is equivalent to doing gradient descent on the squared error between $V(s_t)$ and $r_t + \\gamma V(s_{t+1})$, holding the latter constant:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(V(s_t)) &= \\frac{1}{2} \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right)^2 \\\\\n",
    "\\frac{dL}{dV(s_t)} &= - \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by the gradient descent update rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s_t) &\\gets V(s_t) - \\alpha \\frac{dL}{dV(s_t)} \\\\\n",
    "&= V(s_t) + \\alpha \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right) \\\\\n",
    "&= V(s_t) + \\alpha \\left( r_t + \\gamma V(s_{t+1}) \\right) -  \\alpha  V(s_t) \\\\\n",
    "&= (1 - \\alpha) V(s_t) + \\alpha \\left( r_t + \\gamma V(s_{t+1}) \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This will become more relevant later when the state space $\\mathcal{S}$ is no longer discrete.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an implementation of estimatating $V$ using this update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_V(mrp, T, gamma, alpha, epochs):\n",
    "    V = np.random.rand(mrp.N)*1e-2\n",
    "    for e in range(epochs):\n",
    "        s_t = np.random.choice(mrp.S, p=mrp.p_s_0)\n",
    "        for t in range(T):  \n",
    "            s_t_next = np.random.choice(mrp.S, p=mrp.P[s_t])\n",
    "            r_t = mrp.R[s_t_next]\n",
    "            V[s_t] = (1-alpha)*V[s_t] + alpha*(r_t + gamma*V[s_t_next])\n",
    "            s = s_t_next\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1074364 , 1.9126743 , 2.10367341, 2.02000435, 2.06266409])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_V(mrp, T=10, gamma=0.99, alpha=0.001, epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that these values are similar, since the state transition matrices were originally selected randomly uniformly, so no one state tends to produce consistently bad or consistently good trajectories. \n",
    "\n",
    "Let's force that by making the reward for being in state $0$ relatively high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.R[0] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a custom `P` with high probability of returning to state `0` from state `0`, and low probability of transitioning to state `0` from other states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [np.random.rand(mrp.N) for _ in range(mrp.N)]\n",
    "P[0][0] = 1000\n",
    "for i in range(1, mrp.N):\n",
    "    P[i][0] = 0.001\n",
    "mrp.P = np.vstack([p/p.sum() for p in P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([238.70471114,   3.57678785,   4.09577522,   2.4376589 ,\n",
       "         4.0398525 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_V(mrp, T=10, gamma=0.99, alpha=0.001, epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since trajectories beginning with state $0$ are likely to return to state $0$, and since state $0$ has large reward, we can expect $V(0)$ to be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process $(\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\mathcal{A})$\n",
    "\n",
    "In a Markov process (and by consequence, a Markov reward process), the sequence of states visited during a trajectory is determined entirely by $\\mathcal{P}$. Imagine an agent that lives in the state space $\\mathcal{S}$ and moves from state to state. They have no control over where they visit and are entirely at the mercy of the underlying dynamics of the process.\n",
    "\n",
    "In a Markov decision process (MDP), an agent in a state $s_t$ can influence the probability distribution over next states $s_{t+1}$ by selecting an action $a_t$ from an **action space** $\\mathcal{A}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s_t, a_t, s_{t+1}) = p(s_{t+1} \\mid s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can introduce a notion equivalent to the value function $V(s_t)$, but that measures how good state-action pairs $(s_t, a_t)$ are.\n",
    "$$\n",
    "Q(s_t, a_t) = \\mathbb{E}_\\tau [G_t \\mid s_t, a_t]\n",
    "$$\n",
    "the expected discounted return beginning at state $s_t$ and choosing an action $a_t$. We write the expectation over $\\tau$ as a shorthand for $\\tau \\sim p(\\tau \\mid s_t, a_t, \\mathcal{S})$, which means \"trajectories $\\tau$ sampled from the distribution of trajectories conditional on the initial state $s_t$ and inital action $a_t$, and the state transition matrix $\\mathcal{S}$\".\n",
    "\n",
    "We can expand $G_t$ in the expectation to yield a recursive formulation of $Q(s_t, a_t)$ following the exact same method as for $V(s_t)$, yielding\n",
    "$$\n",
    "Q(s_t, a_t) = \\mathbb{E}_\\tau [r_t + \\gamma Q(s_{t+1}, a_{t+1}) \\mid s_t, a_t] \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtle note: the trajectories $\\tau$ are now generated according to an action $a_t$ at each time step. How can we take the expectation over all trajectories if we don't know how to choose an action at each time step? This is addressed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "An agent living in an MDP needs to have a method for selecting actions $a_t$ given the state $s_t$. Recall the reward hypothesis:\n",
    "\n",
    "> Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal.\n",
    "\n",
    "If we frame this in terms of MDPs, this means that the policy should select actions $a_t$ that maximize the discounted return $G_t$.\n",
    "\n",
    "Policies come in two flavours: deterministic and stochastic. Deterministic policies are often denoted by $\\mu$ and map states directly to actions: $a_t = \\mu(s_t)$. Stochastic policies are denoted by $\\pi$ and map states to a probability distribution over actions $a_t \\sim \\pi(\\cdot \\mid s_t)$. In general, since deterministic policies are just a special case of stochastic policies, we use $\\pi$ when referring to policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a policy $\\pi$ we define\n",
    "$$\n",
    "V^\\pi (s_t) = \\mathbb{E}_\\tau \\left[ G_t \\vert s_t \\right]\n",
    "$$\n",
    "Where the expectation over $\\tau$ is a shorthand for $\\tau \\sim p(\\tau \\mid s_t, a_t, \\pi, \\mathcal{S})$, which means \"trajectories $\\tau$ sampled from the distribution of trajectories conditional on the initial state $s_t$, following state dynamics $\\mathcal{S}$ and action selection policy $\\pi$\".\n",
    "\n",
    "We also likewise define\n",
    "\n",
    "$$Q^\\pi (s_t, a_t) = \\mathbb{E}_\\tau \\left[ G_t \\vert s_t, a_t \\right]$$\n",
    "\n",
    "Where the expectation over $\\tau$ is a shorthand for $\\tau \\sim p(\\tau \\mid s_t, a_t, \\pi, \\mathcal{S})$, which means \"trajectories $\\tau$ sampled from the distribution of trajectories conditional on the initial state $s_t$ and inital action $a_t$, following state dynamics $\\mathcal{S}$ and action selection policy $\\pi$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we implement an MDP and estimate the the Q function for state-action pairs. The `generate_trajectory` method accepts a new parameter `pi` representing the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess(MarkovRewardProcess):\n",
    "    def __init__(self, N, M):\n",
    "        super(MarkovDecisionProcess, self).__init__(N)\n",
    "        self.M = M\n",
    "        # P is now a tensor of shape NxMxN\n",
    "        # P[s, a] is a multinomial distribution\n",
    "        # over next states\n",
    "        P = []\n",
    "        for n in range(N):\n",
    "            p = [np.random.rand(N) for _ in range(M)]\n",
    "            P.append(np.vstack([q/q.sum() for q in p]))\n",
    "        self.P = np.asarray(P)\n",
    "    \n",
    "    def generate_trajectory(self, pi, T):\n",
    "        s_0 = np.random.choice(self.S, p=self.p_s_0)\n",
    "        tau_s = [s_0]\n",
    "        tau_r = []\n",
    "        tau_a = []\n",
    "        s_t = s_0\n",
    "        for t in range(T):\n",
    "            a_t = pi(s_t) # generate action\n",
    "            s_t = np.random.choice(self.S, p=self.P[s_t, a_t])\n",
    "            r_t = self.R[s_t]\n",
    "            tau_s.append(s_t)\n",
    "            tau_r.append(r_t)\n",
    "            tau_a.append(a_t)\n",
    "        return tau_s, tau_r, tau_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MarkovDecisionProcess(5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we use a random uniform policy to demonstrate generating trajectories using an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = lambda s_t: np.random.choice(mdp.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_s, tau_r, tau_pi = mdp.generate_trajectory(pi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3, 2, 4, 2, 2, 3, 0, 3, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44112903184475766,\n",
       " -0.508447116792912,\n",
       " -0.24373616170458026,\n",
       " -1.0738334450260647,\n",
       " -0.24373616170458026,\n",
       " -0.24373616170458026,\n",
       " -0.508447116792912,\n",
       " -0.6787773894547808,\n",
       " -0.508447116792912,\n",
       " -0.6787773894547808]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-Learning\n",
    "\n",
    "A random policy isn't interesting. Let's consider the notion of an **optimal policy**. What does it mean to have an optimal policy? The optimal policy $\\pi^*$ produces trajectories with the highest expected discounted return. Imagine that, for any policy $\\pi$ we can automatically determine $V^\\pi$ and $Q^\\pi$. \n",
    "\n",
    "An optimal policy $\\pi^*$ would then always choose an action $a_t$ at each step which maximizes $Q^{\\pi^*}$.\n",
    "$$\n",
    "\\pi^*(s_t) = \\arg \\max_{a_t} Q^{\\pi^*}(s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call this a **greedy policy**. For this policy to make any sense, we need to have access to $Q^{\\pi^*}$. However, it is difficult to determine $Q^\\pi$ for any policy $\\pi$ since in general an agent will not have access to the underlying state dynamics $\\mathcal{P}$. \n",
    "\n",
    "In general, the $Q^\\pi$ (and $V^\\pi$) functions must be **learned**. For MDPs with small, discrete state spaces and small, discrete action spaces, we can try to use dynamic programming to solve this. This is the exact same approach that we used to learn $V$ for a Markov reward process, except instead of having a parameter for each state $s \\in \\mathcal{S}$, we now have a parameter for each state-action pair $(s, a) \\in \\mathcal{S} \\times \\mathcal{A}$.\n",
    "\n",
    "Authors often omit the $\\pi$ from $Q^\\pi$ and $V^\\pi$ when referring to the learned version of these functions, though some others prefer $\\widehat{Q}^\\pi$ and $\\widehat{V}^\\pi$. For simplicity of notation, we choose to omit the $\\pi$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following series of events:\n",
    "\n",
    "1. You are in a state $s_t$\n",
    "2. You choose an action $a_t$ which maximizes $Q(s_t, a_t)$ (since we are following a greedy policy)\n",
    "3. You get a reward $r_t$\n",
    "4. You transition to the next state $s_{t+1}$\n",
    "\n",
    "At this point, you have a slightly better estimate for $Q^\\pi(s_t, a_t)$ according to the Bellman equation, namely \n",
    "\n",
    "$$\n",
    "r_t + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\n",
    "$$ \n",
    "\n",
    "(since we are following a greedy policy)\n",
    "\n",
    "Update the table to be closer to the better estimate with some learning rate $\\alpha$ \n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\gets (1 - \\alpha)Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly like our previous scenario where we used dynamic programming to learn $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_Q(mdp, pi, T, gamma, alpha, epochs):\n",
    "    Q = np.random.rand(mdp.N, mdp.M)*1e-2\n",
    "    for e in range(epochs):\n",
    "        s_t = np.random.choice(mdp.S, p=mdp.p_s_0)\n",
    "        for t in range(T):\n",
    "            a_t = np.argmax(Q[s_t])\n",
    "            s_t_next = np.random.choice(mdp.S, p=mdp.P[s_t, a_t])\n",
    "            r_t = mdp.R[s_t_next]\n",
    "            a_t_next = np.argmax(Q[s_t_next])\n",
    "            Q[s_t, a_t] = (1-alpha)*Q[s_t, a_t] + alpha*(r_t + gamma*Q[s_t_next, a_t_next])\n",
    "            s = s_t_next\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.96995822, -1.96971622],\n",
       "       [-1.84711799, -1.84809373],\n",
       "       [-1.06378587, -1.06275465],\n",
       "       [-1.76588746, -1.75955787],\n",
       "       [-1.81627675, -1.81858411]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = estimate_Q(mdp, pi, 10, gamma=0.99, alpha=0.001, epochs=10000)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an estimate for $Q$, we can use it in a greedy policy that just always chooses the action $a_t$ for a given state $s_t$ that maximizes the estimated $Q(s_t, a_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_pi = lambda s_t: np.argmax(Q[s_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this policy's return $R_t$ with that of the random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4366.367719843221"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_s, tau_r, tau_pi = mdp.generate_trajectory(pi, 10000)\n",
    "sum(tau_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2704.3221227920258"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_s, tau_r, tau_pi = mdp.generate_trajectory(greedy_pi, 10000)\n",
    "sum(tau_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the greedy policy does much better than the random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next post, we will introduce the OpenAI `gym` module for reinforcement learning and discuss Q-learning and deep-Q-learning in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the jupyter notebook [here]()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
