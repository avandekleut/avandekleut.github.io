{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "permalink: /ddpg/\n",
    "layout: notebook\n",
    "author_profile: true\n",
    "title: Hybrid Learning of Value Functions and Policies: Deep Deterministic Policy Gradients (DDPG) for Continuous Control\n",
    "folder: \"ddpg\"\n",
    "ipynb: \"ddpg.ipynb\"\n",
    "excerpt: ##########################\n",
    "header:\n",
    "  teaser: /assets/ddpg/######################\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous post]({% post_url 2020-07-18-ppo %}) we covered **proximal policy optimization** (PPO). We used a surrogate loss which is a lower bound on the policy performance $\\eta$ which allowed us to use multiple updates per gradient step. We also used clipping to ensure the policy did not stray too far from the old policy at each gradient step and stopped doing gradient steps when their KL divergence exceeds some threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep $Q$-Learning for Continuous Action Spaces\n",
    "\n",
    "When we originally covered [vanilla $Q$-learning]({% post_url 2020-05-26-q-learning %}), we used dynamic programming to learn the $Q$-function using the Bellman Equation. This worked because we had a small discrete state space and a small discrete action space.\n",
    "\n",
    "We then extended $Q$-learning to continuous state spaces by using function approximation to learn the $Q$-function. We used a neural network to do this. Recall that the greedy policy used by (deep) $Q$-learning is\n",
    "\n",
    "$$\n",
    "a_t = \\arg \\max_{a_t} Q^*(s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was still feasible using a neural network with a relatively small action space, since we could just have one output for each action at the end of the neural network and use each of these outputs to estimate $Q^*(s_t, a_t)$. \n",
    "\n",
    "We face a problem when we try to use deep $Q$-learning with continuous action spaces, where actions are represented by real-valued vectors. Solving for the arg max is an expensive inner optimization procedure that would have to be done every time the agent takes a step. While this is hypothetically feasible, we ideally want agents that can perform well in realistic real-time scenarios such as video game playing and robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original [DQN setting]({% post_url 2020-05-31-dqn %}), our neural network took in a state $s_t$ and produced one output for each action in the action space. We did this instead of having the network accept both the state and action to produce a scalar to avoid multiple forward passes through the network (one for each action).\n",
    "\n",
    "However, if we want to use continuous action spaces, we will need to reconfigure our deep $Q$-network to accept both the state $s_t$ and action $a_t$ to produce a scalar output, the prediction for $Q^*(s_t, a_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the action space is continuous, we now assume that the $Q$-function is differentiable with respect to the action as well as the state. We define a **deterministic policy** $\\mu_\\theta$ with parameter vector $\\theta$ that maps states directly to the continuous action space (i.e., $\\mu_\\theta$ is vector-valued). Then, instead of running an inner optimization procedure to select $\\max_{a_t} Q^*(s_t, a_t)$, we approximate\n",
    "\n",
    "$$\n",
    "\\max_{a_t} Q^*(s_t, a_t) \\approx Q^*(s_t, \\mu_\\theta(s_t))\n",
    "$$\n",
    "\n",
    "(i.e., we assume $\\mu_\\theta(s_t)$ is the optimal action). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in DQN, we use a neural network $Q_\\phi$ to estimate $Q^*$. Recall the $Q$-learning loss for DQN:\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\mathbb{E}_\\tau \\left[ \\left(Q_\\phi(s_t, a_t) - \\left(r_t + \\gamma (1 - d_t) \\max_{a_{t+1}} Q_\\phi(s_{t+1}, a_{t+1}) \\right) \\right)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in DQN, we will use a **replay buffer** $\\mathcal{D}$ to store previous experience and stabilize training. \n",
    "\n",
    "We will also use **target networks**. In DQN, we used a target network $Q_{\\phi}^-$ to generate the target value\n",
    "\n",
    "$$\n",
    "r_t + \\gamma (1 - d_t) \\max_{a_{t+1}} Q_\\phi^- (s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "In DQN the target network has frozen parameters that get updated to match those of $Q_\\phi$. Unlike in DQN where we just update the parameters every $n_\\theta$ time steps, we can instead use **Polyak averaging**:\n",
    "\n",
    "$$\n",
    "\\theta^- \\gets \\rho \\theta^- + (1-\\rho) \\theta\n",
    "$$\n",
    "\n",
    "where $\\rho$ is a parameter close to 1. This is essentially a step along a linear interpolation between parameter vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given our approximation $\\max_{a_t} Q^*(s_t, a_t) \\approx Q^*(s_t, \\mu(s_t))$, we update the $Q$-learning loss:\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\mathbb{E}_\\tau \\left[ \\left(Q_\\phi(s_t, a_t) - \\left(r_t + \\gamma (1 - d_t) Q_\\phi^-(s_{t+1}, \\mu_\\theta(s_{t+1}) \\right) \\right)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just how we use target networks for $Q_\\phi$ to stabilize training, we also use target networks for $\\mu_\\theta$ in the target to stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final loss function looks like\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\mathbb{E}_\\tau \\left[ \\left(Q_\\phi(s_t, a_t) - \\left(r_t + \\gamma (1 - d_t) Q_\\phi^-(s_{t+1}, \\mu_\\theta^-(s_{t+1}) \\right) \\right)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mu_\\theta^-$ is the target policy. Again, we us Polyak averaging at each time step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^- \\gets \\rho \\theta^- + (1-\\rho) \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regular policy gradient regime, we train our policy $\\mu_\\theta$ to maximize the expected return:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\mathbb{E}_\\tau \\left[ G_t \\right] \\\\\n",
    "&= \\mathbb{E}_\\tau \\left[ Q^*(s_t, a_t) \\right] \\\\\n",
    "&\\approx \\mathbb{E}_\\tau \\left[ Q_\\phi (s_t, \\mu_\\theta(s_t)) \\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we treat the neural network $Q_\\phi$ as a \"constant\" operation on the action $\\mu_\\theta(s_t)$, then we can just use this neural network to try to maximize the expected return. We call this technique **deep deterministic policy gradients** or DDPG.\n",
    "\n",
    "We learn by interleaving training the $Q_\\phi$ with training the policy $\\mu_\\theta$.\n",
    "\n",
    "Because the $Q$-learning loss can use any state transition, the learning is actually done **off-policy** unlike previous policy gradient methods like REINFORCE, A2C and PPO which are **on-policy** algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in previous posts where continuous control was achieved by outputting the mean of a diagonal Gaussian distribution and then sampling from that, in this algorithm the policy is **deterministic**. This means that initial exploration may be poor, leading to poor learning by the $Q$-network. There are several approaches to solving this, but the easiest is to add Gaussian noise to the action during training to \"smooth out\" the learned $Q$-function, and then remove the noise at testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, batch_size=32, size=1000000):\n",
    "        '''\n",
    "        batch_size (int): number of data points per batch\n",
    "        size (int): size of replay buffer.\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=size)\n",
    "        \n",
    "        self.states = np.zeros()\n",
    "\n",
    "    def remember(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        '''\n",
    "        s_t (np.ndarray double): state\n",
    "        a_t (np.ndarray int): action\n",
    "        r_t (np.ndarray double): reward\n",
    "        d_t (np.ndarray float): done flag\n",
    "        s_t_next (np.ndarray double): next state\n",
    "        '''\n",
    "        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))\n",
    "\n",
    "    def sample(self):\n",
    "        '''\n",
    "        random sampling of data from buffer\n",
    "        '''\n",
    "        # if we don't have enough samples yet\n",
    "        size = min(self.batch_size, len(self.memory))\n",
    "        return random.sample(self.memory, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, num_envs=1):\n",
    "        '''\n",
    "        env (gym.Env): to make copies of\n",
    "        num_envs (int): number of copies\n",
    "        '''\n",
    "        super().__init__(env)\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [copy.deepcopy(env) for n in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Return and reset each environment\n",
    "        '''\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        '''\n",
    "        Take a step in the environment and return the result.\n",
    "        actions (torch.tensor)\n",
    "        '''\n",
    "        next_states, rewards, dones = [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "#             next_state, reward, done, _ = env.step(action.item())\n",
    "            next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "            if done:\n",
    "                next_states.append(env.reset())\n",
    "            else:\n",
    "                next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, env, mu_lr=1e-2, Q_lr=1e-2, gamma=0.99):\n",
    "        '''\n",
    "        env (gym.Env): the environment\n",
    "        lr (float): learning rate\n",
    "        '''\n",
    "        self.N = env.observation_space.shape[0]\n",
    "        self.M = env.action_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # policy network\n",
    "        self.mu = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.N, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, self.M)\n",
    "        ).double()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.normal_(self.mu[-1].weight, mean=0, std=1e-2)\n",
    "            torch.nn.init.normal_(self.mu[-1].bias, mean=0, std=1e-2)\n",
    "        \n",
    "        self.mu_ = copy.deepcopy(self.mu)\n",
    "\n",
    "        self.mu_opt = torch.optim.Adam(self.mu.parameters(), lr=mu_lr)\n",
    "\n",
    "        # Q network\n",
    "        self.Q = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.N+self.M, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        ).double()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.normal_(self.Q[-1].weight, mean=0, std=1e-2)\n",
    "            torch.nn.init.normal_(self.Q[-1].bias, mean=0, std=1e-2)\n",
    "            \n",
    "        self.Q_ = copy.deepcopy(self.Q)\n",
    "\n",
    "        self.Q_opt = torch.optim.Adam(self.Q.parameters(), lr=Q_lr)\n",
    "    \n",
    "    def act(self, s_t, alpha=1, beta=0):\n",
    "        '''\n",
    "        s_t (np.ndarray): the current state\n",
    "        alpha (float): action coefficinet\n",
    "        beta (float): noise coefficient\n",
    "        \n",
    "        alpha=0 beta=1 corresponds to random action\n",
    "        alpha=1 beta=0 corresponds to deterministic action\n",
    "        Because of environment vectorization, this will produce\n",
    "        E actions where E is the number of parallel environments.\n",
    "        '''\n",
    "        s_t = torch.as_tensor(s_t).double()\n",
    "        a_t = self.mu(s_t)\n",
    "        \n",
    "        eps_mu = torch.zeros_like(a_t)\n",
    "        eps_sigma = torch.ones_like(a_t)\n",
    "        eps = torch.distributions.Normal(eps_mu, eps_sigma).sample()\n",
    "        \n",
    "        return alpha*a_t + beta*eps\n",
    "    \n",
    "    def synchronize(self, A, B, rho):\n",
    "        '''\n",
    "        Shift A towards B by (1-rho)\n",
    "        '''\n",
    "        for a, b in zip(A, B):\n",
    "            if hasattr(a, 'weight'):\n",
    "                a.weight.data = rho*a.weight.data + (1-rho)*b.weight.data\n",
    "            if hasattr(a, 'bias'):\n",
    "                a.bias.data = rho*a.bias.data + (1-rho)*b.bias.data\n",
    "                \n",
    "    def synchronize_Q(self, rho=0.99):\n",
    "        self.synchronize(self.Q_, self.Q, rho)\n",
    "        \n",
    "    def synchronize_mu(self, rho=0.99):\n",
    "        self.synchronize(self.mu_, self.mu, rho)\n",
    "        \n",
    "    def learn_Q(self, states, actions, rewards, dones, next_states):\n",
    "        '''\n",
    "        ################# \n",
    "        '''\n",
    "\n",
    "        states = torch.as_tensor(states)\n",
    "        actions = torch.as_tensor(actions)\n",
    "        rewards = torch.as_tensor(rewards)\n",
    "        dones = torch.as_tensor(dones)\n",
    "        next_states = torch.as_tensor(next_states)\n",
    "        \n",
    "#         print(f'''\n",
    "#         states.shape: {states.shape}\n",
    "#         actions.shape: {actions.shape}\n",
    "#         rewards.shape: {rewards.shape}\n",
    "#         dones.shape: {dones.shape}\n",
    "#         ''')\n",
    "\n",
    "        next_actions = self.mu_(states).detach()\n",
    "        target_inputs = torch.cat((next_states, next_actions), dim=-1)\n",
    "        \n",
    "#         print(f'''\n",
    "#         target_inputs.shape: {target_inputs.shape}\n",
    "#         ''')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Q_targ = self.Q_(target_inputs)\n",
    "#             print(f'''\n",
    "#             Q_targ.shape: {Q_targ.shape}\n",
    "#             ''')\n",
    "            Q_targ = Q_targ.squeeze()\n",
    "#             print(f'''\n",
    "#             Q_targ.shape (squeezed): {Q_targ.shape}\n",
    "#             ''')\n",
    "            target = rewards + (1-dones)*self.gamma*Q_targ\n",
    "            \n",
    "#         print(f'''\n",
    "#         target.shape: {target.shape}\n",
    "#         target: {target}\n",
    "#         ''')\n",
    "\n",
    "        inputs = torch.cat((states, actions), dim=-1)\n",
    "        Q_pred = self.Q(inputs).squeeze()\n",
    "        \n",
    "#         print(f'''\n",
    "#         Q_pred.shape: {Q_pred.shape}\n",
    "#         Q_pred: {Q_pred}\n",
    "#         ''')\n",
    "        \n",
    "#         print(f'''\n",
    "#         diff: {target - Q_pred}\n",
    "#         ''')\n",
    "        \n",
    "        loss = torch.mean((target - Q_pred)**2)\n",
    "        \n",
    "#         print(f'''\n",
    "#         loss: {loss}\n",
    "#         ''')\n",
    "\n",
    "        self.Q_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Q_opt.step()\n",
    "    \n",
    "    def learn_pi(self, states):\n",
    "        '''\n",
    "        #####################\n",
    "        '''\n",
    "        states = torch.as_tensor(states)\n",
    "        actions = self.mu(states)\n",
    "\n",
    "        inputs = torch.cat((states, actions), dim=-1)\n",
    "        Q_pred = self.Q(inputs).squeeze()\n",
    "        loss = torch.mean(-Q_pred)\n",
    "\n",
    "        self.Q_opt.zero_grad()\n",
    "        self.mu_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.mu_opt.step()\n",
    "        \n",
    "    def update(self, states, actions, rewards, dones, next_states, rho):\n",
    "        states = torch.as_tensor(states).double()\n",
    "        actions = torch.as_tensor(actions).double()\n",
    "        rewards = torch.as_tensor(rewards).double()\n",
    "        dones = torch.as_tensor(dones).double()\n",
    "        next_states = torch.as_tensor(next_states).double()\n",
    "        \n",
    "        \n",
    "        self.learn_Q(states, actions, rewards, dones, next_states)\n",
    "        self.learn_pi(states)\n",
    "        self.synchronize_Q(rho)\n",
    "        self.synchronize_mu(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "\n",
    "def DDPG(env, agent, \n",
    "        gamma=0.99,\n",
    "         rho=0.99,\n",
    "         batch_size=128,\n",
    "         start_steps=1000,\n",
    "         T=4052):    \n",
    "    \n",
    "    states = np.empty((T, env.num_envs, agent.N))\n",
    "    actions = np.empty((T, env.num_envs, agent.M))\n",
    "    rewards = np.empty((T, env.num_envs))\n",
    "    dones = np.empty((T, env.num_envs))\n",
    "    next_states = np.empty((T, env.num_envs, agent.N))\n",
    "    \n",
    "    # for plotting\n",
    "    totals = []\n",
    "    episode_rewards = 0\n",
    "    \n",
    "    s_t = env.reset()\n",
    "    \n",
    "    for t in range(start_steps):\n",
    "        a_t = agent.act(s_t, 0, 1) # random action\n",
    "        s_t_next, r_t, d_t = env.step(a_t)\n",
    "\n",
    "        states[t] = s_t\n",
    "        actions[t] = a_t.detach().numpy()\n",
    "        rewards[t] = r_t\n",
    "        dones[t] = d_t\n",
    "        next_states[t] = s_t_next\n",
    "        \n",
    "        s_t = s_t_next\n",
    "    \n",
    "    for t in range(start_steps, T):\n",
    "        env[0].render()\n",
    "        a_t = agent.act(s_t, 1, 0.99**t)\n",
    "        s_t_next, r_t, d_t = env.step(a_t)\n",
    "\n",
    "        states[t] = s_t\n",
    "        actions[t] = a_t.detach().numpy()\n",
    "        rewards[t] = r_t\n",
    "        dones[t] = d_t\n",
    "        next_states[t] = s_t_next\n",
    "        \n",
    "        batch_indices = np.random.randint(low=0, high=min(t, batch_size), size=batch_size)\n",
    "\n",
    "        assert not np.isnan(states[batch_indices]).any()\n",
    "        assert not np.isnan(actions[batch_indices]).any()\n",
    "        assert not np.isnan(rewards[batch_indices]).any()\n",
    "        assert not np.isnan(dones[batch_indices]).any()\n",
    "        assert not np.isnan(next_states[batch_indices]).any()\n",
    "        \n",
    "        agent.update(\n",
    "            states[batch_indices],\n",
    "            actions[batch_indices],\n",
    "            rewards[batch_indices],\n",
    "            dones[batch_indices],\n",
    "            next_states[batch_indices],\n",
    "            rho\n",
    "        )\n",
    "\n",
    "        episode_rewards += r_t\n",
    "        for i in range(env.num_envs):\n",
    "            if d_t[i]:\n",
    "                totals.append(episode_rewards[i])\n",
    "                episode_rewards[i] = 0\n",
    "                print(totals[-1])\n",
    "                \n",
    "        s_t = s_t_next\n",
    "        \n",
    "    sns.lineplot(x=range(len(totals)), y=totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorizedEnvWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-952a73f021b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym_cartpole_swingup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizedEnvWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPoleSwingUp-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorizedEnvWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "import gym_cartpole_swingup\n",
    "env = VectorizedEnvWrapper(gym.make(\"CartPoleSwingUp-v0\"), num_envs=8)\n",
    "agent = Policy(env, mu_lr=0.01, Q_lr=0.01, gamma=0.999)\n",
    "DDPG(env, agent, start_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
